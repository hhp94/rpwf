---
title: "rpwf"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{rpwf}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  warning = FALSE
)
```

```{r, echo = FALSE}
tmp_dir <- withr::local_tempdir(pattern = "vignette")
# setwd("~/project/rpwf")
setwd(tmp_dir)
```

```{r setup}
library(rpwf)
library(dplyr)
library(parsnip)
library(recipes)
```

## Introduction
* I created this package because I love data wrangling in R, i.e. `{data.table}`,
`{tidyverse}`, spline basis functions generations with `{mgcv}` and `{gratia}`, 
and visualization with `{ggplot2}` to name a few...  
  + Many new methods of performing dimension reduction are first implemented
  in R because they are developed by statisticians.  
* Meanwhile, the ML and DL packages in python are usually more mature, more 
flexible, and a lot of the latest updates and ML research papers have python 
implementations.  
  + I find that fitting model in python is usually faster and consume less 
  memory for the equivalent model in R.  
  + Importantly, ML acceleration with GPU can be more straight-forward to setup
  by less experienced people (like myself) in python.  
* With {rpwf}, I can wrangle my data in R while having access to the latest and 
greatest ML packages in python.  
* In Kaggle competitions, it is usually feature engineering, rather than testing
different ML models, that gives a competitor an edge. {rpwf} helps by  
  + Enabling the use of the `{workflowset}` framework that sets up multiple 
  test experiments and sequentially, or in parallel on different compute nodes, 
  evaluate them to test if an engineered feature helps with the prediction
  performance.  
  + Allowing the seamless deployment on HPC clusters or cloud computers because 
  you only have to upload the original train data and recipes to run the 
  experiments. All the pathing and uploading of results is handled by (a) SQLite 
  database(s).  

## Demonstration
* Let's demonstrate using the [Wisconsin Diagnostic Breast Cancer (WDBC)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)
data by Wolberg et. al. 1994 hosted by the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).  

* Here's a description of the data
> Features are computed from a digitized image of a fine needle
> aspirate (FNA) of a breast mass.  They describe
> characteristics of the cell nuclei present in the image. 

* Using these features, the task is to classify each image into malignant or 
benign cells, recorded in the `diagnosis` column.  

```{r}
if (!file.exists("./wdbc.data")) {
  download.file(
    url = paste(
      "https://archive.ics.uci.edu/ml",
      "machine-learning-databases/breast-cancer-wisconsin/wdbc.data",
      sep = "/"
    ),
    destfile = "./wdbc.data"
  )
}
```

* Here are the feature descriptions, each have 3 statistics, mean, standard error, 
and "largest" (mean of three largest values).  
	a) radius (mean of distances from center to points on the perimeter)
	b) texture (standard deviation of gray-scale values)
	c) perimeter
	d) area
	e) smoothness (local variation in radius lengths)radius
	f) compactness (perimeter^2 / area - 1.0)
	g) concavity (severity of concave portions of the contour)
	h) concave points (number of concave portions of the contour)
	i) symmetry 
	j) fractal dimension ("coastline approximation" - 1)

```{r}
names_df <- expand.grid(
  variable = c(
    "radius", "texture", "perimeter", "area", "smoothness", "compactness",
    "concavity", "concave_points", "symmetry", "fractal_dimension"
  ),
  stat = c("mean", "se", "worst")
) |>
  dplyr::arrange(variable, stat)
names_df$names <- paste0(names_df$variable, "_", names_df$stat)

df <- read.csv("./wdbc.data", header = FALSE)
names(df) <- c("id", "diagnosis", names_df$names)
```

```{r}
df[1:6, 1:6]
```

* Data is balanced. 
```{r}
table(df$diagnosis)
```

* And although there's no missing data, the use of packages such as [{missForest}](https://cran.r-project.org/web/packages/missForest/) 
and [{mice}](https://cran.r-project.org/web/packages/mice/) to perform multiple 
imputation and propagate that uncertainty into the predictive performance 
estimation is a strong point of R.  
```{r}
sum(is.na(df))
```

* Recode malignant as 1 and benign as 0.  
```{r}
df$diagnosis <- ifelse(df$diagnosis == "M", 1L, 0L)
```

## Initialize a database
* Create a database with `rpwf_connect_db()`. We name this database `"db.SQLite"`.
```{r}
db_con <- rpwf_connect_db("db.SQLite", tmp_dir)
```

* `db_con` is an object that holds a `{DBI}` connection to the SQLite database. 
Access it with the `$con` method  
```{r}
db_con$con
```

## Define models with `{parsnip}`
* Identical to the steps in parsnips, first choose a model, i.e.,
`boost_tree()`, then choose the R engine with `set_engine()` and classification 
or regression with `set_mode()`.  
* Then, just pipe the object into the `set_py_engine()` function.  
* `set_py_engine()` has 3 important arguments  
  + `py_module` and `py_base_learner` defines how to import a base learner in 
  a python script.  
  + `args` defines the arguments that can be passed to the base learner in python.  
    + **Make sure to pass Boolean as R Boolean**, i.e. `TRUE` and `FALSE`.  
    + This is a powerful feature. It allows us to fit models using the latest 
    python implementation from R. For example, the `xgboost` module allows 
    setting monotonicity constraints and different base learners such as linear 
    learners and random forest learners instead of the default learner.  
  + `tag` is an optional argument that's helpful for keeping track of models in 
  complicated `workflowsets`.  
* Tip: Unless you are expecting bugs or testing, try to pass arguments that
make the learner as silent as possible. Some ML packages in python generate a 
lot of messages. For example, even with the `verbosity = 0` and `silent = TRUE` 
from the [xgboost docs](https://xgboost.readthedocs.io/en/stable/python/python_api.html?highlight=n_trees#module-xgboost.sklearn), the output from the python script still has a lot of 
repeated messages and deprecation warnings.  

### `{xgboost}`
* These are the available models  
```{r}
rpwf_avail_models(db_con$con)
```

* From the [xgboost docs](https://xgboost.readthedocs.io/en/stable/python/python_api.html?highlight=n_trees#module-xgboost.sklearn), I decided to fix the `n_estimators`
at 50 and tune the learning rate.  
* To do this, I pass the parameter `n_estimators = 50` to the `args` argument
of `set_py_engine()`.  
* I am going to tune 6 hyper parameters by passing them the `tune()` functions 
just like in parsnips.  

```{r}
# This model is equivalent to the following python codes:

# from xgboost import XGBClassifier
#
# base_learner = XGBClassifier(
#   eval_metric = "logloss",
#   use_label_encoder = False,
#   verbosity = 0,
#   silent = True
# )

xgb_model <- boost_tree(
  mtry = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune()
) |>
  set_engine("xgboost") |>
  set_mode("classification") |>
  set_py_engine(
    "xgboost",
    "XGBClassifier",
    args = list(
      eval_metric = "logloss",
      n_estimators = 50,
      use_label_encoder = FALSE,
      verbosity = 0,
      silent = TRUE
    )
  )
```

### `svm`
* From the [sklearn.svm.SCV docs](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), the argument `cache_size` can help speed up the fit for users if RAM
space is available. I will increase this from the default. This is an example
of how fitting models in python can have some very useful settings.  
  + In this example, `cache_size` wouldn't reduce fit time because the data is 
  small.  
* Let's set up one model for *polynomial kernel*, one for *radial basis kernel*.  
  + I fixed the `kernel` argument in `args` to explicitly separate the models 
  into poly and rbf.  
  + This is an artifact of `{tidymodels}` defining `svm_poly()` and `svm_rbf()` 
  separately while sci-kit learn define them both in one class 
  `sklearn.svm.SVC`.  

```{r}
svm_poly_model <- svm_poly(
  cost = tune(),
  degree = tune(),
  scale_factor = tune()
) |>
  set_engine("kernlab") |>
  set_mode("classification") |>
  set_py_engine("sklearn.svm",
    "SVC",
    "svm_poly",
    tag = "svm_poly",
    args = list(
      kernel = "poly",
      cache_size = 500
    )
  )

svm_rbf_model <- svm_rbf(
  cost = tune(),
  rbf_sigma = tune()
) |>
  set_engine("kernlab") |>
  set_mode("classification") |>
  set_py_engine("sklearn.svm",
    "SVC",
    "svm_rbf",
    tag = "svm_rbf",
    args = list(
      kernel = "rbf",
      cache_size = 500
    )
  )
```

## Hyper parameter tuning
* The `{dials}` package provides 1) sensible hyper parameters ranges, and 2) 
functions that go beyond the random grid and regular grids such as 
`dials::grid_max_entropy`, and `dials::grid_latin_hypercube`.  
* The `dials::grid_latin_hypercube` will be helpful for models with a lot of 
hyper parameters such as `xgboost_model`. But for `svm_rbf_model`, tuning just 2 
parameters on a 2-D grid with regular grid would work best.  
* I can add **model specific tuning grid** at this step with `set_r_grid()`. But 
if I want just one type of grid for all my models I can do this step later on.  
  + For the `xgboost_model`, I add used a `dials::grid_latin_hypercube`.  
```{r}
xgb_model <- xgb_model |>
  set_r_grid(dials::grid_latin_hypercube, size = 100)
```
  + For `svm_poly_model`, I use a grid regular, but only 2 polynomial degrees of 
  2 and 3.  
```{r}
svm_poly_model <- svm_poly_model |>
  set_r_grid(dials::grid_regular, levels = 10, filter = degree %in% c(2, 3))
```
  + For `svm_rbf_model`, I just use a 2D regular grid.    
```{r}
svm_rbf_model <- svm_rbf_model |>
  set_r_grid(dials::grid_regular, levels = 12)
```

# Define recipe with `{recipes}`  
* Recipes are defined just as a normal `tidymodels` workflow.  
  + Use formula or the role interface to indicate the response and predictors.  
  + The base recipe is used to gauge the baseline performance of each model.  
  + The pca recipe is used to de-correlate the variables. `step_pca()` conveniently
  provides an argument to keep an arbitrary threshold of the variance explained. 
  I choose 95%.  
* `recipes` are data transformation pipelines. 
* `rpwf` reserves one special role that can be used with the `update_role()` 
function:  
  + `pd.index` is a special role. It will mark a column for conversion into
  pandas index in python.  
  + Below, the column `df$id` will become the `pandas.DataFrame` Index and not 
  a predictor.  
* Complicated workflow sets can get complicated quickly. Add `rpwf_tag_recipe()`  
to recipe to add a description to the recipe.  
```{r}
common <- recipe(diagnosis ~ ., data = df) |>
  update_role(id, new_role = "pd.index")

xgb_base_rec <- common |>
  rpwf_tag_recipe("xgb_base")

svm_base_rec <- common |>
  step_normalize() |>
  rpwf_tag_recipe("svm_base")

xgb_pca_rec <- xgb_base_rec |>
  step_pca(threshold = .95) |>
  rpwf_tag_recipe("xgb_pca")

svm_pca_rec <- svm_base_rec |>
  step_pca(threshold = .95) |>
  rpwf_tag_recipe("svm_pca")
```

## Create workflowset  
* The function `rpwf_workflow_set()` mimics [`workflowsets::workflow_set()`](https://workflowsets.tidymodels.org/). It 
create a combination of all the provided recipes and models. Then, one can 
work with the resulting data.frame just like any data.frame by filtering 
out redundant workflows and etc. I can also create a `workflow_set` for xgboost
and one for svm and `rbind()` them.   
* The cost argument is to specify which cost function to optimize by. Look up
the values in the [scikit-learn docs](https://scikit-learn.org/stable/modules/model_evaluation.html). 
Custom cost functions are possible but would require coding on the python side.    

```{r}
xgb_wfs <- rpwf_workflow_set(
  preprocs = list(xgb_base_rec, xgb_pca_rec),
  models = list(xgb_model),
  cost = "roc_auc"
)

svm_wfs <- rpwf_workflow_set(
  preprocs = list(svm_base_rec, svm_pca_rec),
  models = list(svm_poly_model, svm_rbf_model),
  cost = "roc_auc"
)

all_wfs <- rbind(xgb_wfs, svm_wfs)
```

* Then, add necessary information with `rpwf_augment()`  
```{r}
all_wfs <- rpwf_augment(all_wfs, db_con)
all_wfs |>
  dplyr::select(model_tag, recipe_tag, costs)
```

* Checking the generated grids, we can see that the names of the hyper 
parameters have been renamed to conform to the scikit-learn API.  
```{r}
sapply(unique(all_wfs$grids), head)
```

## Export data as parquets and add to database.
* There are two types of parquets files 1) hyper param grids, and 2) train data
frame.  
* `rpwf_export_grid()` and `rpwf_export_df()` write the parquets and returns
id columns that track where the parquets are to the workflow set.  
* Because this function only generate a data.frame if its not already written, 
running each of these functions in parallel is not recommended.  
  + To get around this, one can either work with a manageable number of 
  workflow at a time, or split the work into multiple different databases and 
  run the export functions for each database in parallel.    
```{r}
exported_wfs <- all_wfs |>
  rpwf_export_grid(db_con) |>
  rpwf_export_df(db_con)
```

* Then finally, export the meta data into the database with `rpwf_export_db()`.  
```{r}
rpwf_export_db(exported_wfs, db_con$con)
```

## Run the workflow in python
* Navigate to the local installation of the `rpwf` python codes from the 
instructions in the [README.md](https://github.com/hhp94/rpwf/blob/master/README.md) 
file.  
* `rpwf` python codes contains scripts that performs model fitting. These are 
also templates to experiment further using the data generated in R.   
* For example, to use the `nested_resampling.py` script, in the terminal, run 
the following command to get the list of arguments  
```{bash, eval = FALSE}
python -m rpwf.script.nested_resampling -h
```

* Or the following in a Jupyter notebook cell
```{jupyter, eval = FALSE}
%run -m rpwf.script.nested_resampling -h
```

* This following command display the workflows we exported.  
  + The positional argument is the path to the database. In this case, my temp 
  folder.  
  + The `-db` flag is to specify which database are we connecting to.  
  + The `-s` flag shows the workflows.  
* The other important flags are  
  + The `-a` flag runs all the workflow that hasn't been run.    
  + The `-w` flag accept a list of ids (i.e., 1 2 3 4) to specify which workflow
  to run.  
  + The `-f` flag force a run and overwrite the results of a workflow that has
  already been run.  
  + The `-c` flag indicates the number of CPU cores dedicated to the model fitting
  task.  
```{bash, eval = FALSE}
python -m rpwf.script.nested_resampling "/tmp/RtmpNMsSNR/vignette44bb4c7d4218" \
   -db "db.SQLite" -s
```

* Let's run the nested CV with 5 CV for the inner loop for hyper param tuning,
5 CV * 5 Repeats on the outer loop for performance validation.  
```{bash, eval = FALSE}
python -m rpwf.script.nested_resampling "/tmp/RtmpNMsSNR/vignette44bb4c7d4218" \
   -db "db.SQLite" -a -c 7 -icv 5 -ocv 5 - ocr 5
```

