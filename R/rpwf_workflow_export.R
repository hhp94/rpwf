#' Customized Version of `{workflowsets}`
#'
#' Wrapper around [tidyr::crossing()] that creates all combinations of recipes
#' and models.
#'
#' @param preprocs list or vector of [recipes::recipe()].
#' @param models list or vector of model spec. Generated by adding
#' [set_py_engine()] to a model, e.g. [parsnip::boost_tree()] and
#' [parsnip::set_engine()].
#' @param costs list or vector of sklearn cost optimization metrics such as
#' "neg_log_loss" and "roc_auc".
#'
#' @return tibble that contains a combination of list of recipes, models,
#' and costs.
#' @export
#' @examples
#' d <- rpwf_sim_()$train
#' m1 <- parsnip::boost_tree() |>
#'   parsnip::set_engine("xgboost") |>
#'   parsnip::set_mode("classification") |>
#'   set_py_engine(py_module = "xgboost", py_base_learner = "XGBClassifier")
#' r1 <- d |>
#'   recipes::recipe(target ~ .) |>
#'   recipes::step_dummy(.data$X3, one_hot = TRUE) |>
#'   # "pd.index" is the special column that used for indexing in pandas
#'   recipes::update_role(.data$id, new_role = "pd.index")
#' wf <- rpwf_workflow_set(list(r1), list(m1), "neg_log_loss")
#' wf
rpwf_workflow_set <- function(preprocs, models, costs) {
  stopifnot(is.vector(preprocs) & is.vector(models) & is.vector(costs))
  list_class_fns <- function(list) {
    c <- unique(sapply(list, class))
    return(c)
  }

  stopifnot("preproc accept list of recipes" = "recipe" == list_class_fns(preprocs))
  stopifnot("models accept list of models" = "model_spec" %in% list_class_fns(models) &
    !"recipe" %in% list_class_fns(models))
  stopifnot("costs accept list of characters" = "character" %in% list_class_fns(costs))

  df <- tidyr::crossing(
    preprocs = unique(preprocs), models = unique(models), costs = unique(costs)
  )

  df$costs <- as.character(costs) # results is a list, this unlist that

  return(df)
}

#' Add a Tag to a Recipe
#'
#' Complicated workflow sets can become difficult to track. Add a tag to keep
#' track of unique recipes.
#'
#' @param obj An [recipes::recipe()] object.
#' @param tag Short description of recipe.
#'
#' @return A tagged [recipes::recipe()] object. Accessible with `obj$recipe_tag`.
#' @export
#'
#' @examples
#' r <- recipes::recipe(~., data = mtcars)
#' r <- rpwf_tag_recipe(r, "test recipe")
#' r$recipe_tag
rpwf_tag_recipe <- function(obj, tag) {
  obj$recipe_tag <- tag
  return(obj)
}

#' Helper Function for Running Query to Add Ids to the `workflow_set`
#'
#' This function is a wrapper for [DBI::dbGetQuery()] that also perform some
#' cleaning. Should only return one column and <= 1 row for every query.
#'
#' @param query SQL query.
#' @inheritParams rpwf_dm
#' @param ... value vectors to search the tables by. Accept up to 3 values.
#'
#' @return a vector of query values.
#' @keywords internal
#' @export
rpwf_query_ <- function(query, con, ...) {
  val_vecs <- list(...)
  query_res_list <- purrr::pmap(
    .l = val_vecs,
    \(val1, val2 = NULL, val3 = NULL) {
      val <- as.list(c(val1, val2, val3))
      DBI::dbGetQuery(con, glue::glue_sql(query, .con = con), val)
    }
  )

  stopifnot(
    "Query should only return 1 column and <= 1 row" =
      all(sapply(query_res_list, \(query) {
        ncol(query) == 1L & nrow(query) <= 1L
      }))
  )
  # Number of rows for each query
  empty_res <- sapply(query_res_list, \(query) {
    nrow(query)
  })
  # If any query finds nothing (nrow(query) == 0) then replace with NA
  query_res_list[which(empty_res == 0L)] <- NA
  return(as.vector(unlist(query_res_list))) # return an unlisted vector for mutate
}

#' Add Model Related Columns to the `workflow_set`
#'
#' @param obj output of [rpwf_workflow_set()].
#' @inheritParams rpwf_dm
#'
#' @return tibble that contains 3 additional columns "py_base_learner",
#' "py_base_learner_args", "model_mode".
#' @keywords internal
#' @export
rpwf_add_model_info_ <- function(obj, con) {
  stopifnot(
    "Run rpwf_workflow_set() first!" =
      all(c("preprocs", "models", "costs") %in% names(obj))
  )
  rpwf <- obj
  # Add the py module column
  rpwf$py_module <- sapply(rpwf$models, \(x) {
    x$py_module
  })
  # Add the py_base_learner column
  rpwf$py_base_learner <- sapply(rpwf$models, \(x) {
    x$py_base_learner
  })
  # Add the r engine column
  rpwf$engine <- sapply(rpwf$models, \(x) {
    x$engine
  })
  # Check for invalid models
  purrr::pwalk(
    .l = list(
      x = rpwf$py_module, y = rpwf$py_base_learner, z = rpwf$engine
    ),
    .f = \(x, y, z) {
      rpwf_chk_model_avail_(con, x, y, z)
    }
  )
  # Query the rename dictionary json
  hyper_par_rename <- rpwf_query_(
    query = "SELECT hyper_par_rename FROM model_type_tbl WHERE
    py_module = ? AND py_base_learner = ? AND r_engine = ?; ",
    con = con,
    val1 = rpwf$py_module,
    val2 = rpwf$py_base_learner,
    val3 = rpwf$engine
  )
  # Add the rename fns by passing the dictionary to rpwf_grid_rename_()
  rpwf$rename_fns <- lapply(hyper_par_rename, rpwf_grid_rename_)

  # Add the base learner related args if presented
  rpwf$py_base_learner_args <- vapply(rpwf$models, \(x) {
    if (!is.null(x$py_base_learner_args)) {
      return(x$py_base_learner_args)
    } else {
      return(NA_character_)
    }
  }, "character")

  # get the model mode (regression/classification)
  rpwf$model_mode <- sapply(rpwf$models, \(x) {
    x$mode
  })
  return(rpwf)
}

#' Adds a Short Description of Each Workflow to the `workflow_set`
#'
#' @param obj object generated by [rpwf_add_model_info_()].
#' @return tibble that contains the additional column "model_tag" and
#' "recipe_tag".
#' @keywords internal
#' @export
rpwf_add_desc_ <- function(obj) {
  stopifnot("Run rpwf_add_model_info_ first!" = "py_base_learner" %in% names(obj))
  # Add the model tag column
  obj$model_tag <- vapply(obj$models, \(x) {
    if (is.null(x$model_tag)) {
      return(x$py_base_learner)
    } else {
      return(x$model_tag)
    }
  }, "character")

  # Add the recipe tag column
  obj$recipe_tag <- vapply(obj$preprocs, \(x) {
    if (is.null(x$recipe_tag)) {
      return(NA_character_)
    } else {
      return(x$recipe_tag)
    }
  }, "character")

  return(obj)
}

#' Add Relevant Parameters to the `dials::grid_<functions>`
#'
#' @param obj object generated by [rpwf_add_model_info_()].
#' @param seed random seed.
#' @inheritParams rpwf_grid_gen_
#'
#' @details
#' Custom grids can be any functions that accept at least an argument "x", as
#' the functions in {dials} do. So for example, this function would work:
#'
#' `function(x){invisible(x); return(mtcars)}`
#'
#' @importFrom rlang .data
#' @return tibble with the additional column "grids".
#' @keywords internal
#' @export
rpwf_add_grid_param_ <- function(obj, .grid_fun = NULL, seed, ...) {
  # These are columns from rpwf_workflow_set()
  stopifnot(
    "Run rpwf_workflow_set() first!" =
      all(c("preprocs", "models", "costs") %in% names(obj))
  )
  dplyr::mutate(obj, grids = purrr::pmap(
    .l = list(.data$models, .data$preprocs, .data$rename_fns),
    .f = \(x, y, z) {
      set.seed(seed)
      rpwf_grid_gen_(
        model = x,
        preproc = y,
        rename_fns = z,
        .grid_fun = .grid_fun,
        ...
      )
    }
  ))
}

#' Query the `model_type_id` for the Requested Model
#'
#' @param obj an object generated by [rpwf_add_model_info_()].
#' @inheritParams rpwf_dm
#'
#' @return tibble with the "model_type_id" column added.
#' @keywords internal
#' @export
rpwf_add_model_type_ <- function(obj, con) {
  stopifnot(
    "add set_py_engine() to your {parsnip} model_spec object" =
      all(c("py_module", "py_base_learner") %in% names(obj))
  )

  query_res <- rpwf_query_(
    query =
      "SELECT model_type_id FROM model_type_tbl
      WHERE py_module = ? AND py_base_learner = ?",
    con = con,
    val1 = obj$py_module,
    val2 = obj$py_base_learner
  )
  # Add `model_type_id` to the accumulating object
  return(dplyr::mutate(obj, model_type_id = query_res))
}


#' Add the Random State Seeds for Python `random_state`
#'
#' @param obj an object generated by [rpwf_add_model_info_()].
#' @param range range of seed to sample from.
#' @param seed random seed to sample the `random_state`.
#'
#' @return tibble with "random_state" column added.
#' @keywords internal
#' @export
rpwf_add_random_state_ <- function(obj, range, seed) {
  set.seed(seed)
  stopifnot("range of random_state should be of length 2" = length(range) == 2L)
  sorted_range <- as.integer(sort(range))
  stopifnot("range should be an int vector" = !anyNA(sorted_range))
  random_state <- sample(sorted_range[1]:sorted_range[2], size = nrow(obj))
  # Add `random_state` to the accumulating object
  return(dplyr::mutate(obj, random_state = random_state))
}

#' Wrapper to Generate the Object to be Exported to the Database
#'
#' @param wflow_obj object created by the [rpwf_workflow_set()] function.
#' @inheritParams rpwf_write_grid
#' @inheritParams rpwf_grid_gen_
#' @inheritParams rpwf_add_random_state_
#'
#' @return tibble with the columns necessary for exporting to db.
#' @export
#' @examples
#' # Create the database
#' temp_dir <- withr::local_tempdir()
#' db_con <- rpwf_connect_db("db.SQLite", temp_dir)
#'
#' # Create a `workflow_set`
#' d <- rpwf_sim_()$train
#' m1 <- parsnip::boost_tree() |>
#'   parsnip::set_engine("xgboost") |>
#'   parsnip::set_mode("classification") |>
#'   set_py_engine(py_module = "xgboost", py_base_learner = "XGBClassifier")
#' r1 <- d |>
#'   recipes::recipe(target ~ .) |>
#'   recipes::step_dummy(.data$X3, one_hot = TRUE) |>
#'   # "pd.index" is the special column that used for indexing in pandas
#'   recipes::update_role(.data$id, new_role = "pd.index")
#' wf <- rpwf_workflow_set(list(r1), list(m1), "neg_log_loss")
#'
#' to_export <- wf |>
#'   rpwf_augment(db_con, dials::grid_latin_hypercube, size = 10)
#' list.files(paste0(temp_dir, "/rpwfDb"), recursive = TRUE) # Files are created
rpwf_augment <- function(wflow_obj, db_con, .grid_fun = NULL,
                         ..., range = c(1L, 5000L), seed = 1234L) {
  py_module <- py_base_learner <- engine <- rename_fns <- model_mode <- NULL
  set.seed(seed)
  wflow_obj |>
    rpwf_add_model_info_(db_con$con) |>
    rpwf_add_desc_() |>
    rpwf_add_grid_param_(.grid_fun, seed, ...) |>
    rpwf_add_model_type_(db_con$con) |>
    rpwf_add_random_state_(range, seed) |>
    dplyr::select(-c(py_module, py_base_learner, engine, rename_fns, model_mode))
}


#' Write the Hyper Param Grid Parquet File
#'
#' This function does the heavy lifting of generating the path,
#' write the parquet, updating the database, for the provided R grids. Since the
#' function is reading from the database and only generate data as needed,
#' running this function in parallel is not recommended.
#'
#' @param obj obj generated by [rpwf_add_model_info_()].
#' @param db_con a [DbCon] object.
#'
#' @details
#' For each grid, initialize a new [RGrid], call `self$export()`, then return
#' the object. This make sure the same object called twice will just fetch
#' the result from the previous call. Hence, can't be run in parallel.
#'
#' At this point, we made sure that 1) the db is updated, 2) file is exported,
#' 3) file exists. Use the hashes for getting the grid id.
#'
#' @return Parquet files.
#' @export
rpwf_write_grid <- function(obj, db_con) {
  stopifnot("run rpwf_add_grid_param_() first" = "grids" %in% names(obj))
  for (g in obj$grids) {
    RGrid$new(g, db_con)$export()
  }
}

#' Write the Train/Test Data
#'
#' Similar to [rpwf_write_grid()], this function does the heavy lifting of
#' generating the path, write the parquet, and updating the database for the
#' recipe of the transformed data. Since the function is reading from the
#' database and only generating data as needed, running this function in parallel
#' is not recommended.
#'
#' @param obj obj generated by [rpwf_add_model_info_()].
#' @inheritParams rpwf_write_grid
#' @param seed random seed. To control for recipes such as down sampling.
#'
#' @details
#' For each recipe, initialize a new [TrainDf], call `self$export()`, then return
#' the object. This make sure the same object called twice will just fetch the
#' result from the previous call.
#'
#' @return Parquet files.
#' @export
rpwf_write_df <- function(obj, db_con, seed = 1234) {
  for (r in obj$preprocs) {
    set.seed(seed)
    TrainDf$new(r, db_con)$export()
  }
}

#' Add Grid Id and Df Id to workflow/data set
#'
#' @param obj augmented workflow set or data set
#' @inheritParams rpwf_write_grid
#'
#' @return A tibble with df_id and also grid_id added.
#' @keywords internal
#' @export
rpwf_parquet_id_ <- function(obj, db_con) {
  path_chk_ <- function(R6_obj, root = db_con$proj_root_path) {
    sapply(R6_obj, \(x) {
      file.exists(paste(root, x$path, sep = "/"))
    })
  }
  #### Get grid id
  RGrid_obj <-
    lapply(obj$grids, \(x) {
      RGrid$new(x, db_con)
    })

  grid_query <- rpwf_query_(
    query = "SELECT grid_id FROM r_grid_tbl WHERE grid_hash = ?",
    con = db_con$con,
    val1 = sapply(RGrid_obj, \(x) {
      x$hash
    })
  )
  stopifnot("grid id not found, `rpwf_write_grid()` first?" = !anyNA(grid_query))
  stopifnot("grid parquet not found, `rpwf_write_grid()` first?" = all(
    path_chk_(RGrid_obj[which(grid_query != 1)]) # Don't check grid if id = 1
  ))
  obj$grid_id <- grid_query

  #### Get df id
  TrainDf_obj <- lapply(obj$preprocs, \(x) {
    TrainDf$new(x, db_con)
  })
  print(sapply(TrainDf_obj, \(x) {
    x$hash
  }))

  df_query <- rpwf_query_(
    con = db_con$con,
    query = "SELECT df_id FROM df_tbl WHERE df_hash = ?",
    val1 = sapply(TrainDf_obj, \(x) {
      x$hash
    })
  )
  # print(df_query)
  stopifnot("df id not found, `rpwf_write_df()` first?" = !anyNA(df_query))
  stopifnot("df parquet not found, `rpwf_write_df()` first?" = all(path_chk_(TrainDf_obj)))
  obj$df_id <- df_query

  return(obj)
}

#' Hash the Rows of the Workflow to Check if its Already in the db
#'
#' Export the object that has been processed by the other functions into the
#' database so that python can query the information.
#'
#' @param df object created by [rpwf_augment()].
#'
#' @return a vector of [rlang::hash].
#' @keywords internal
#' @export
rpwf_wflow_hash_ <- function(df) {
  apply(as.data.frame(df), 1, rlang::hash)
}

#' Generate the Export to DB functions
#'
#' @param required_col String of required columns.
#'
#' @return a function that add the required columns to the database.
#' @noRd
rpwf_export_fns_ <- function(required_cols) {
  fns <- function(obj, db_con) {
    rpwf <- rpwf_parquet_id_(obj = obj, db_con = db_con)

    # These columns must be present
    required <- force(required_cols)

    # Query the wflow that's already in the database
    db_wflow_hash <- rpwf_wflow_hash_(
      dplyr::select(
        DBI::dbGetQuery(db_con$con, glue::glue("SELECT * FROM wflow_tbl;")),
        dplyr::all_of(required)
      )
    )

    # Generate hash of current wflows
    to_export_hash <- rpwf_wflow_hash_(dplyr::select(rpwf, dplyr::all_of(required)))
    matched_wflow <- to_export_hash %in% db_wflow_hash

    if (any(matched_wflow)) {
      message("the following workflows are already in the database\n")
      print(rpwf[matched_wflow, which(names(rpwf) %in% required)])
    }
    # Only add the workflow that's not in the database
    to_export <- as.data.frame(rpwf[!matched_wflow, which(names(rpwf) %in% required)])

    if (nrow(to_export) == 0) {
      message("All workflows found in db, exiting...")
      return(0)
    } else {
      message("Exporting workflows to db...")
      DBI::dbAppendTable(db_con$con, name = "wflow_tbl", value = to_export)
    }
  }
  return(fns)
}

#' Export the [rpwf_augment()] Object into the Database
#'
#' @param obj object generated by [rpwf_augment()].
#' @inheritParams rpwf_write_grid
#'
#' @return number of rows exported.
#' @export
#' @examples
#' # Create the database
#' temp_dir <- withr::local_tempdir()
#' db_con <- rpwf_connect_db("db.SQLite", temp_dir)
#'
#' # Create a `workflow_set`
#' d <- mtcars
#' d$target <- stats::rbinom(nrow(d), 1, 0.5)
#' m1 <- parsnip::boost_tree() |>
#'   parsnip::set_engine("xgboost") |>
#'   parsnip::set_mode("classification") |>
#'   set_py_engine("xgboost", "XGBClassifier", "my_xgboost_model")
#' r1 <- d |>
#'   recipes::recipe(target ~ .)
#' wf <- rpwf_workflow_set(list(r1), list(m1), "neg_log_loss")
#'
#' to_export <- wf |>
#'   rpwf_augment(db_con, dials::grid_latin_hypercube, size = 10)
#' rpwf_write_grid(to_export, db_con)
#' rpwf_write_df(to_export, db_con)
#'
#' # Before exporting
#' DBI::dbGetQuery(db_con$con, "SELECT * FROM wflow_tbl;")
#' # After exporting
#' rpwf_export_wfs(to_export, db_con)
#' DBI::dbGetQuery(db_con$con, "SELECT * FROM wflow_tbl;")
rpwf_export_wfs <- rpwf_export_fns_(
  c(
    "df_id",
    "grid_id",
    "model_tag",
    "recipe_tag",
    "costs",
    "model_type_id",
    "random_state",
    "py_base_learner_args"
  )
)

#' Export the Locations of Train/Test Parquets into the Database
#'
#' @param obj object generated by [rpwf_df_set].
#' @inheritParams rpwf_write_grid
#'
#' @return number of rows exported.
#' @export
#' @examples
#' # Create the database
#' temp_dir <- withr::local_tempdir()
#' db_con <- rpwf_connect_db("db.SQLite", temp_dir)
#'
#' d <- mtcars
#' d$target <- stats::rbinom(nrow(d), 1, 0.5)
#' r1 <- d |>
#'   recipes::recipe(target ~ .)
#' df <- rpwf_df_set(r1)
#'
#' rpwf_write_df(df, db_con)
#'
#' # Before exporting
#' DBI::dbGetQuery(db_con$con, "SELECT * FROM wflow_tbl;")
#' # After exporting
#' rpwf_export_dfs(df, db_con)
#' DBI::dbGetQuery(db_con$con, "SELECT * FROM wflow_tbl;")
rpwf_export_dfs <- rpwf_export_fns_(c("df_id", "grid_id", "recipe_tag"))
