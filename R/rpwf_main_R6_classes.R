# BaseEx -----------------------------------------------------------------------
#' @title Internal R6 Object Only for Setting up Inheritance
#'
#' @description
#' A R6 object that provides a common set of methods to 1) store the metadata of
#' data in the db, and 2) export parquet files for python to import. Not
#' meant to be called manually.
#' @keywords internal
#'
#' @export BaseEx
BaseEx <- R6::R6Class(
  "BaseEx",
  public = list(
    #' @field hash (`character()`)\cr
    #' hash of the object by [rlang::hash()].
    hash = NULL,
    #' @field path (`character()`)\cr
    #' path to the file in the project folder.
    path = NULL,
    #' @field df (`data.frame()`)\cr
    #' the df for the hyper param grid or the transformed df.
    df = NULL,
    #' @field con (`DBI::dbConnect()`)\cr
    #' a [DBI::dbConnect()] object, created by [DbCon].
    con = NULL,
    #' @field export_query (`glue::glue_sql()`)\cr
    #' SQL query to upload metadata of the data into the db.
    export_query = NULL,
    #' @field queried_path (`glue::glue_sql()`)\cr
    #' SQL query results to check if metadata of the object already exists in
    #' the db.
    queried_path = NULL,
    #' @field proj_root_path (`character()`)\cr
    #' root path of the project, using `here::here()` is recommended.
    proj_root_path = NULL,
    #' @field db_folder (`character()`)\cr
    #' the hyper param grid and transformed df are stored in separate folders.
    #' This attribute holds the name of that folder.
    db_folder = NULL,
    #' @field db_name name of the database.
    db_name = NULL,

    #' @description
    #' This class is the parent of RGrid and TrainDf R6 object, not meant to be
    #' called.
    #' @param db_con (`DbCon`)\cr
    #' a [DbCon] object.
    initialize = function(db_con) {
      stopifnot("input should be a R6 DbCon() object" = R6::is.R6(db_con))
      self$path <- NULL
      self$df <- NULL
      self$con <- db_con$con
      self$db_name <- db_con$db_name
      self$hash <- NULL
      self$export_query <- NULL
      self$queried_path <- NULL
      self$db_folder <- NULL
      self$proj_root_path <- db_con$proj_root_path
    },

    #' @description
    #' Run the query that check if the object exists in the db by checking the
    #' hash of the object in the db. Either find one unique row or no row.
    #' @param query (`glue::glue_sql()`)\cr
    #' SQL query for either the hyper parameter grid or the transformed df.
    exec_query = function(query) {
      DBI::dbGetQuery(self$con, query, list(self$hash))
    },

    #' @description
    #' Pass the hash of the object to this function to set the hash attribute.
    #' @param val (`any`)\cr
    #' New hash, either an integer or string.
    set_hash = function(val) {
      self$hash <- as.character(val)
    },

    #' @description
    #' Change the path where the object is stored. Is NA only when no grid is
    #' provided. Have to provide a train data.
    #' @param val (`character()`)\cr
    #' Path to store the object on disk.
    set_path = function(val) {
      self$path <- val
    },

    #' @description
    #' Change the folder name that store the object.
    #' @param val (`character()`)\cr
    #' New folder names.
    set_db_folder = function(val) {
      self$db_folder <- val
    },

    #' @description
    #' Store the results of the query from the db in a data.frame. Query have to
    #' return a df (so [nrow()] works) with 0 <= `nrow()` < 2.
    #' @param val (`data.frame()`)\cr
    #' New data frame generated by `self$exec_query`.
    find_path_in_db = function(val) {
      stopifnot(is.data.frame(val) & nrow(val) < 2)
      self$queried_path <- val
    },

    #' @description
    #' Change the SQL query to export metadata to database.
    #' @param val (`glue::glue_sql()`)\cr
    #' New SQL query to export metadata.
    set_export_query = function(val) {
      self$export_query <- val
    },

    #' @description
    #' If the hash of the new object is not found in the database, then new
    #' data is prepared. If the data is found in the metadata but not in the
    #' indicated path then new data is also prepared. Otherwise, `self$df` is
    #' NULL and will be skipped by the `BaseEx::self$export_parquet()` method.
    #' @param val (`data.frame()`)\cr
    #' Either a [recipes::juice()] object or a data.frame of the hyper param grid.
    #' @param type (`character()`)\cr
    #' Used in message
    set_df = function(val, type) {
      withr::local_dir(new = self$proj_root_path)
      # find_path_in_db must be run first
      if (nrow(self$queried_path) == 0L) {
        # If query yields 0 rows, then create df
        # message(glue::glue("Preparing new {type}..."))
        self$df <- val
      } else if (!is.na(self$queried_path$path) &
        !file.exists(self$queried_path$path)) {
        # If parquet file not found but is in found in database
        # message(glue::glue("Metadata found, but new {type} is needed..."))
        self$df <- val
      } # Otherwise no transformation needed, leave `self$df` as NULL
      else {
        # message(glue::glue("{type} parquet found in {self$db_folder}"))
      }
    },

    #' @description
    #' If the `self$export_query` is generated because metadata is not found in
    #' the database, then export the metadata to the db using this query.
    #' Otherwise return NULL.
    export_db = function() {
      if (!is.null(self$export_query)) {
        # message("Exporting to db")
        DBI::dbExecute(self$con, self$export_query) # if an export query is created
        self$set_export_query(NULL) # then run the query and reset to NULL
      }
      invisible(self)
    },

    #' @description
    #' If the file specified by `self$path` is not found, then export the file
    #' as a parquet file to the location specified in the metadata.
    export_parquet = function() {
      withr::local_dir(self$proj_root_path)
      if (is.na(self$path)) {
        message("No grid provided\n")
      } else if (!file.exists(self$path)) {
        # message("Writing parquet file\n")
        arrow::write_parquet(self$df, self$path)
      } else {
        # message("Parquet file found\n")
      }
      invisible(self)
    },

    #' @description
    #' Wrapper around exporting information to the db and writing the parquet
    #' file.
    export = function() {
      self$export_db()$export_parquet()
    },

    #' @description
    #' If the db query found no existing metadata, then an export path is made.
    #' If the metadata is found but the associated data is not found, then an
    #' export path is also made. Else get the path from the metadata in the db.
    #' @param new_path (`character()`)\cr
    #' Path to the object.
    #' @param new_export_query (`glue::glue_sql()`)\cr
    #' SQL query to export the obj metadata to the db.
    export_prep = function(new_path, new_export_query) {
      # If the query generate no entry, then make a path
      if (nrow(self$queried_path) == 0L) {
        self$set_path(new_path)
        self$set_export_query(new_export_query)
      } else {
        # Else get the path from the query results and assign to `self$path`
        self$set_path(self$queried_path$path)
      }
    },

    #' @description
    #' Create the "rpwfDb" folder to store the parquet if it doesn't exists.
    create_folder = function() {
      ## Create folder if not exists
      withr::local_dir(self$proj_root_path)
      folder <- paste("rpwfDb", self$db_folder, sep = "/")
      if (!dir.exists(folder)) {
        # message(glue::glue("Creating {folder} folder..."))
        dir.create(folder)
      }
    }
  )
)


# TrainDf ----------------------------------------------------------------------
#' @title Internal R6 Object that Process the Data Transformation
#'
#' @description
#' A R6 object that manage the export of metadata and parquet file of the
#' transformed data defined by the recipe for data transformation. Accept a
#' special role `"pd.index"` from the [recipes::update_role()] as an index for
#' a pandas DataFrame. If no outcome is provided, then the `data.frame` is
#' considered a "test" `data.frame.`
#'
#' @details
#' This object works by:
#' + inherits from the [BaseEx] class.
#' + accept a recipe and generate a prepped object with [recipes::prep()].
#' + use the prepped object to get the name of the pd.index, target, and
#' predictors.
#' + calculate the hash of the prepped recipe (not the data frame) and check
#' the hash of the prepped recipe in the database.
#' + if the hash is found in the database:
#'   + assign the path associated with the hash of the data.frame to `self$path`.
#'   + check if the file exists with the `self$path`.
#'   + if not exists, transform the prepped object with [recipes::juice()] and
#'    assign to `self$df`.
#'   + if the file exists, then assign `NULL` to `self$df` attribute to stop
#'   `self$export_parquet()` from executing.
#' + if the hash is not found in the database:
#'   + transform the prepped object with [recipes::juice()] and assign to
#'   `self$df`.
#'   + generate a new path to write the transformed data.
#'   + generate a SQL query to update the database with the new hash and new path
#' + update the database with the generated SQL query with `self$export_db()`
#' + write the parquet with `self$export_parquet()`
#' @keywords internal
#'
#' @export TrainDf
TrainDf <- R6::R6Class(
  "TrainDf",
  inherit = BaseEx,
  public = list(
    #' @field prepped (`recipes::prep()`)\cr
    #' holds the prepped object.
    prepped = NULL,
    #' @field term_info (`dplyr::tibble()`)\cr
    #' the `self$prepped` object has the attribute `term_info` that has
    #' information of transformed variable before actually transforming the data.
    term_info = NULL,
    #' @field idx_col (`character()`)\cr
    #' Having a pre-defined index in R makes working with `pandas.DataFrame` less
    #' error prone. Defined by the provided recipe.
    idx_col = NULL,
    #' @field target (`character()`)\cr
    #' Name of the target variable. If missing, then a message is returned to say
    #' that a test df is assumed to be generated. Defined by the provided recipe.
    target = NULL,
    #' @field predictors (`character()`)\cr
    #' List of names of the predictors. Stored as JSON string to be parsed in
    #' python into a python list.
    predictors = NULL,
    # Holds the recipe to transform the data.

    #' @description
    #' Create a new instance of the TrainDf class. Accept a
    #' special role `"pd.index"` from the [recipes::update_role()] as an index for
    #' a pandas DataFrame. If no outcome is provided, then the `data.frame` is
    #' considered a "test" `data.frame.`. See `?rprw::BaseEx` for
    #' details about the attributes and methods.
    #' @param recipe (`recipes::recipe()`)\cr
    #' provided recipe that defines how the data is transformed.
    #' @param db_con (`DbCon`)\cr
    #' a [DbCon] object.
    initialize = function(recipe, db_con) {
      stopifnot("a recipe object required" = "recipe" %in% class(recipe))

      super$initialize(db_con) # Init from the super class
      self$prepped <- recipes::prep(recipe) # Init the prepped object
      self$term_info <- self$prepped$term_info # post-transform train metadata
      self$set_hash(rlang::hash(self$prepped)) # Set the hash of the prepped obj
      self$set_db_folder(glue::glue("{self$db_name}_df")) # Set the root folder
      self$create_folder() # Create the folder if needed
      # use the hash of the prepped to find the path
      self$find_path_in_db(self$exec_query(
        glue::glue_sql("SELECT df_path AS path FROM df_tbl WHERE df_hash = ?",
          .con = self$con
        )
      ))
      self$set_idx_col() # set index column for pandas
      self$set_target_col() # set target column for pandas
      self$set_predictors() # get the predictors
      self$set_df(recipes::juice(self$prepped), "transformed data")
      self$export_prep(
        new_path = glue::glue(
          "rpwfDb", "{self$db_folder}", "{self$hash}.df.parquet",
          .sep = "/"
        ),
        new_export_query = glue::glue_sql(
          "INSERT INTO df_tbl (idx_col, target, predictors, df_path, df_hash)
            VALUES ({vals*})",
          vals = c(
            self$idx_col, self$target, self$predictors, self$path, self$hash
          ), .con = self$con
        )
      ) # Get the path for parquet file
    },

    #' @description
    #' Set the index column as defined by the recipe.
    set_idx_col = function() {
      idx <- which(self$term_info$role == "pd.index") # which var is "index"
      if (length(idx) == 1) {
        self$idx_col <- self$term_info[idx, "variable", drop = TRUE]
        message(glue::glue("Adding {self$idx_col} as pandas idx"))
      } else {
        message("No pandas idx added. Use update_roles() with 'pd.index' for one")
        self$idx_col <- NA
      }
      invisible(self)
    },

    #' @description
    #' Set the target column as defined by the recipe. Assume to be test data
    #' if the target is not found.
    set_target_col = function() {
      targ <- which(self$term_info$role == "outcome") # which var is "outcome"
      if (length(targ) == 1) {
        self$target <- self$term_info[targ, "variable", drop = TRUE]
      } else {
        message("No outcome added. Add in recipe, or assuming this is test data")
        self$target <- NA
      }
      invisible(self)
    },

    #' @description
    #' Store the list of predictors defined by the recipe as a json string
    #' to be parsed in python.
    set_predictors = function() {
      pred <- which(self$term_info$role == "predictor") # which are predictors
      stopifnot("Must have more than 0 predictors" = length(pred) > 0)
      self$predictors <- as.character(
        jsonlite::toJSON(self$term_info[pred, "variable", drop = TRUE])
      )
    }
  )
)


# RGrid ------------------------------------------------------------------------
#' @title Internal R6 object that process the hyper param grid generated in R
#'
#' @description
#' A R6 object that manage the export of metadata and parquet file of the
#' hyper param grid generated in R
#'
#' @details
#' This object works by:
#' + inherits from the [BaseEx] class.
#' + accept a hyper param grid generated with [rpwf_grid_gen_].
#' + calculate the hash of the grid and check the hash is in the database.
#' + if the hash is found in the database:
#'   + assign the path associated with the hash of the grid to `self$path`.
#'   + check if the grid exists with `self$path`.
#'   + if not exists, assign the grid to `self$df`.
#'   + if the file exists, then assign `NULL` to the `self$df` attribute to stop
#'   `self$export_parquet()` from executing.
#' + if the hash is not found in the database:
#'   + assign the grid to `self$df`.
#'   + generate a new path to write the grid.
#'   + generate a SQL query to update the database with the new hash and new path
#' + update the database with the generated SQL query with `self$export_db()`
#' + write the parquet with `self$export_parquet()`
#' @keywords internal
#'
#' @export RGrid
RGrid <- R6::R6Class(
  "RGrid",
  inherit = BaseEx,
  public = list(
    #' @description
    #' Create a new instance of the RGrid class. These are grids made by
    #' functions such as `dials::grid_latin_hypercube` and `dials::grid_regular`.
    #' See [BaseEx] for details of the attributes.
    #' @param grid_obj (`rpwf::rpwf_grid_gen_()`)\cr
    #' [rpwf_grid_gen_()] performs necessary clean ups before the grid can
    #' be used in python.
    #' @param db_con (`DbCon`)\cr
    #' a [DbCon] object.
    initialize = function(grid_obj, db_con) {
      super$initialize(db_con) # Init from the super class
      self$set_hash(rlang::hash(grid_obj)) # hash the grid
      self$set_db_folder(glue::glue("{self$db_name}_grid")) # Set the root folder to "rpwf_grids"
      self$create_folder() # Create the folder if needed
      self$find_path_in_db(self$exec_query(
        glue::glue_sql( # hash is passed into ?
          "SELECT grid_path AS path FROM r_grid_tbl WHERE grid_hash = ?",
          .con = self$con
        )
      ))
      self$set_df(grid_obj, "hyper param grid")
      self$export_prep(
        new_path = glue::glue(
          "rpwfDb", "{self$db_folder}", "{self$hash}.grid.parquet",
          .sep = "/"
        ),
        new_export_query = glue::glue_sql(
          "INSERT INTO r_grid_tbl (grid_path, grid_hash) VALUES ({vals*})",
          vals = c(self$path, self$hash),
          .con = self$con
        ) # Get the path for parquet file
      )
    }
  )
)


# Functions related to hyper param grids ---------------------------------------

#' Create a Tune Grid Specific to the Provided Model Spec
#'
#' The hyper parameter grid can either be provided with this function or all at
#' once later on with [rpwf_augment()]
#'
#' @inheritParams set_py_engine
#' @param .model_grid_fun a `dials::grid_<functions>`, e.g., `grid_random()`,
#' `grid_latin_hypercube()`.
#' @param .model_update_params update {dials} functions such as [dials::penalty()]
#' and [dials::cost()] with a named list. See examples.
#' @param ... arguments to pass to `.model_grid_fun`.
#'
#' @return Add the `{dials}` function and associated arguments to the model spec
#' @export
#'
#' @examples
#' m <- parsnip::boost_tree() |>
#'   parsnip::set_engine("xgboost") |>
#'   parsnip::set_mode("classification") |>
#'   set_r_grid(dials::grid_random, list(trees = dials::trees(range = c(2, 3))), size = 5)
#' m$.model_grid_fun
#' m$.model_grid_fun_args
set_r_grid <- function(obj, .model_grid_fun, .model_update_params = NULL, ...) {
  stopifnot(".model_grid_fun needs to be function" = is.function(.model_grid_fun))
  obj$.model_grid_fun <- .model_grid_fun
  obj$.model_update_params <- .model_update_params
  obj$.model_grid_fun_args <- rlang::exprs(...)
  return(obj)
}

#' A wrapper around `dials::grid_<functions>` to create a sklearn suitable grid
#'
#' This function takes whatever grid functions that are available in `{dials}`,
#' converts it to a sklearn suitable format by renaming the hyper parameters as
#' well as adding fixed parameters to overwrite the default values in sklearn if
#' provided and perform some cleaning such as converting R `mtry` counts to
#' Python `col_by_sample` proportion.
#'
#' @param model a model spec object defined by `{parsnip}`.
#' @param preproc a recipe object defined by `{recipes}`.
#' @param rename_fns a function to rename the grid, generated by
#' [rpwf_grid_rename_()]
#' @param .grid_fun a `dials::grid_<functions>`, e.g., `grid_random()`,
#' `grid_latin_hypercube()`. Default `NULL` assumes that no grid search is
#' performed and default params in sklearn are used. See details for custom grids.
#' @param ... additional arguments for the `.grid_fun` functions.
#'
#' @return a `rpwf_grid` object, which is just a modified `{dials}` grid but
#' made suitable for sklearn.
#' @keywords internal
#' @export
rpwf_grid_gen_ <- function(model,
                           preproc,
                           rename_fns,
                           .grid_fun = NULL,
                           update_params = NULL,
                           ...) {
  params <- rpwf_finalize_params_(model = model, preproc = preproc)
  fun <- NULL
  update_list <- NULL

  # Assign these two variables. Make sure the engine specific argument overide
  # the arguements here.
  if (!is.null(model$.model_grid_fun)) {
    fun <- model$.model_grid_fun
  } else {
    fun <- .grid_fun
  }

  if (!is.null(model$.model_update_params)) {
    update_list <- model$.model_update_params
  } else {
    update_list <- update_params
  }

  if (!is.null(model$.model_grid_fun_args)) {
    grid_args <- model$.model_grid_fun_args
  } else {
    grid_args <- list(...)
  }

  if (nrow(params$pars) == 0 | is.null(fun)) {
    message("No hyper param tuning specified")
    return(NA) # If hash of NA is changed, has to update rpwf_db_init_values_()
  }
  stopifnot(".grid_fun needs to be function" = is.function(fun))

  if (!is.null(update_list)) {
    params$pars <- do.call("update", c(list(params$pars), update_list))
  }

  if (!is.null(fun)) {
    r_grid <- do.call(
      fun,
      c(list(x = params$pars), grid_args)
    )
  }

  return(
    rpwf_transform_grid_(r_grid, rename_fns = rename_fns, n_predictors = params$n_predictors)
  )
}

#' Transform the Hyper Parameter Grid
#'
#' Some hyper parameters between R and Python are transformed differently. This
#' function is a temporary solution for transformation before a more scalable
#' solution specific to each model is needed.
#'
#' @param grid the generated R grid
#' @inheritParams rpwf_grid_gen_
#' @param ... other parameters needed for transformation
#'
#' @return a transformed and renamed grid
#' @keywords internal
#' @export
rpwf_transform_grid_ <- function(grid, rename_fns, n_predictors) {
  r_grid <- dplyr::rename_with(grid, rename_fns)

  # if ("max_depth" %in% colnames(r_grid)) {
  #   message("high value of 'max_depth' can cause memory error")
  # }

  if ("colsample_bytree" %in% colnames(r_grid)) {
    message("'colsample_bytree' is detected. Converting to proportions")
    # `colsample_bytree` is mtry converted into proportion so we need a denominator.
    #  Denominator is number is number of predictors
    r_grid$colsample_bytree <- round(r_grid$colsample_bytree / n_predictors, 3)
  }

  if ("C" %in% colnames(r_grid)) {
    message("'C' is detected. Reciprocating")
    r_grid$C <- 1 / r_grid$C
  }

  return(r_grid)
}


#' Generate Functions to Rename Hyper Params to sklearn API Suitable Names
#'
#' sklearn uses different names for the same hyper params in `{parsnips}`. This
#' function create functions to rename the names generated by
#' `dials::grid_functions` into python suitable names of the hyper parameters
#' that requires tuning. Called by [dplyr::rename_with()] in [rpwf_grid_gen_()].
#'
#' @param json json that defines how to rename the parameters.
#'
#' @return a function that returns a string of the corresponding hyper param
#' in sklearn.
#' @keywords internal
#' @export
#'
#' @examples
#' rename_fns <- rpwf_grid_rename_(jsonlite::toJSON(list("mtry" = "colsample_bytree")))
#' rename_fns(c("mtry", "not_added"))
rpwf_grid_rename_ <- function(json) {
  dict <- force(jsonlite::fromJSON(json))
  get_name <- function(x) {
    value <- dict[[x]]
    if (is.null(value)) {
      return(x)
    } else {
      return(value)
    }
  }
  # This function is returned to be used with rename_with()
  fns <- function(x) {
    res <- vapply(x, get_name, "character")
    names(res) <- NULL
    return(res)
  }
}

#' Internal Function to Finalize the Parameters that Requires Train Data
#'
#' This function creates the finalized parameter object in R
#'
#' @inheritParams rpwf_grid_gen_
#'
#' @return finalized parameter object.
#' @keywords internal
#' @export
rpwf_finalize_params_ <- function(model, preproc) {
  # some parameters (mtry) requires the data to be finalized
  stopifnot("model_spec" %in% class(model) &
    "recipe" %in% class(preproc))

  # Get just the predictors of pre-transformed data
  preds <- preproc$var_info[
    which(preproc$var_info$role == "predictor"), "variable",
    drop = TRUE
  ]

  # mtry conversion to python requires ncol() and nrow() of pre-transform data
  finalized_params <- dials::finalize(
    hardhat::extract_parameter_set_dials(model),
    dplyr::select(preproc$template, dplyr::all_of(preds))
  )

  # prevent specifying of params not belonging to the model in R. Works by
  # raising an subscript out of range error if the param not found.
  tryCatch(
    {
      labels <- sapply(finalized_params$object, \(x) {
        x[["label"]]
      })
    },
    error = function(c) {
      message("Tuning params not found")
      stop(c)
    }
  )
  return(list(pars = finalized_params, n_predictors = length(preds)))
}
