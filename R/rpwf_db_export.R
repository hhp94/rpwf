#' Customized Version of `{workflowsets}`
#'
#' Wrapper around [tidyr::crossing()] that creates all combinations of recipes
#' and models.
#'
#' @param preprocs list or vector of [recipes::recipe()].
#' @param models list or vector of model spec. Generated by adding
#' [set_py_engine()] to a model, e.g. [parsnip::boost_tree()] and
#' [parsnip::set_engine()].
#' @param costs list or vector of sklearn cost optimization metrics such as
#' "neg_log_loss" and "roc_auc", add more by adding rows to the SQL `cost_tbl`.
#'
#' @return tibble that contains a combination of list of recipes, models,
#' and costs.
#' @export
#' @examples
#' d <- rpwf_sim()$train
#' m1 <- parsnip::boost_tree() |>
#'   parsnip::set_engine("xgboost") |>
#'   parsnip::set_mode("classification") |>
#'   set_py_engine(py_module = "xgboost", py_base_learner = "XGBClassifier")
#' r1 <- d |>
#'   recipes::recipe(target ~ .) |>
#'   recipes::step_dummy(.data$X3, one_hot = TRUE) |>
#'   # "pd.index" is the special column that used for indexing in pandas
#'   recipes::update_role(.data$id, new_role = "pd.index")
#' wf <- rpwf_workflow_set(list(r1), list(m1), "neg_log_loss")
#' wf
rpwf_workflow_set <- function(preprocs, models, costs) {
  stopifnot(is.vector(preprocs) & is.vector(models) & is.vector(costs))
  return(
    tidyr::crossing(
      preprocs = unique(preprocs), models = unique(models), costs = unique(costs)
    )
  )
}

#' Add `py_base_learner`, `py_base_learner_args`, `model_mode` to the `workflow_set`
#'
#' @param obj output of [rpwf_workflow_set()].
#' @inheritParams rpwf_dm_obj
#'
#' @return tibble that contains 3 additional columns "py_base_learner",
#' "py_base_learner_args", "model_mode".
#' @keywords internal
#' @export
rpwf_add_model_info <- function(obj, con) {
  stopifnot(
    "Run rpwf_workflow_set() first!" =
      all(c("preprocs", "models", "costs") %in% names(obj))
  )
  rpwf <- obj
  # Add the r engine column
  rpwf$engine <- vapply(rpwf$models, \(x) {
    x$engine
  }, "character")
  # Add the py module column
  rpwf$py_module <- vapply(rpwf$models, \(x) {
    x$py_module
  }, "character")
  # Add the py base learner column
  rpwf$py_base_learner <- vapply(rpwf$models, \(x) {
    x$py_base_learner
  }, "character")

  # Check for invalid models
  purrr::pwalk(
    .l = list(
      x = rpwf$py_module, y = rpwf$py_base_learner, z = rpwf$engine
    ),
    .f = \(x, y, z) {
      rpwf_chk_model_avail(con, x, y, z)
    }
  )

  # Add the base learner related args if presented
  rpwf$py_base_learner_args <- vapply(rpwf$models, \(x) {
    if (!is.null(x$py_base_learner_args)) {
      return(x$py_base_learner_args)
    } else {
      return(NA_character_)
    }
  }, "character")

  # get the model mode (regression/classification)
  rpwf$model_mode <- vapply(rpwf$models, \(x) {
    x$mode
  }, "character")
  return(rpwf)
}

#' Helper Function for Running Query to Add ids to the `workflow_set`
#'
#' This function is a wrapper for [DBI::dbGetQuery()] that also perform some
#' cleaning. Should only return one column and <= 1 row for every query.
#'
#' @param query SQL query.
#' @inheritParams rpwf_dm_obj
#' @param val1 value vector to search the tables by.
#' @param val2 value vector to search the tables by.
#'
#' @return a vector of query values.
#' @keywords internal
#' @export
rpwf_query <- function(query, con, val1, val2 = NULL) {
  if (is.null(val2)) {
    query_res_list <- lapply(
      val1,
      \(x) {
        DBI::dbGetQuery(con, glue::glue_sql(query, .con = con), list(x))
      }
    )
  } else {
    query_res_list <- purrr::map2(
      val1, val2,
      \(x, y) {
        DBI::dbGetQuery(con, glue::glue_sql(query, .con = con), list(x, y))
      }
    )
  }
  stopifnot(
    "Query should only return 1 column and <= 1 row" =
      all(sapply(query_res_list, \(query) {
        ncol(query) == 1L & nrow(query) <= 1L
      }))
  )
  # Number of rows for each query
  empty_res <- sapply(query_res_list, \(query) {
    nrow(query)
  })
  # If any query finds nothing (nrow(query) == 0) then replace with NA
  query_res_list[which(empty_res == 0L)] <- NA
  return(as.vector(unlist(query_res_list))) # return an unlisted vector for mutate
}

#' Adds a Short Description of Each Workflow to the `workflow_set`
#'
#' @param obj object generated by [rpwf_add_model_info()].
#' @return tibble that contains the additional column "wflow_desc".
#' @importFrom rlang .data
#' @keywords internal
#' @export
rpwf_add_desc <- function(obj) {
  stopifnot("Run rpwf_add_model_info first!" = "py_base_learner" %in% names(obj))
  # by pasting together preprocs, models and costs cols
  return(dplyr::mutate(obj, wflow_desc = paste(
    names(.data$preprocs), .data$py_base_learner, .data$costs,
    sep = "-"
  )))
}

#' Add Relevant Parameters to the `dials::grid_<functions>`
#'
#' @param obj object generated by [rpwf_add_model_info()].
#' @param seed random seed.
#' @inheritParams rpwf_grid_gen
#'
#' @details
#' Custom grids can be any functions that accept at least an argument "x", as
#' the functions in {dials} do. So for example, this function would work:
#'
#' `function(x){invisible(x); return(mtcars)}`
#'
#' @importFrom rlang .data
#' @return tibble with the additional column "grids".
#' @keywords internal
#' @export
rpwf_add_grid_param <- function(obj, .grid_fun = NULL, seed, ...) {
  # These are columns from rpwf_workflow_set()
  stopifnot(
    "Run rpwf_workflow_set() first!" =
      all(c("preprocs", "models", "costs") %in% names(obj))
  )
  dplyr::mutate(obj, grids = purrr::map2(
    .data$models, .data$preprocs, \(x, y) {
      set.seed(seed)
      rpwf_grid_gen(model = x, preproc = y, .grid_fun = .grid_fun, ...)
    }
  ))
}

#' Add the RGrid R6 Objects Using the Provided Grids.
#'
#' This object does the heavy lifting of generating the path,
#' parquets, and updating the database of the provided R grids. Since the
#' function is reading from the database and only generating data as needed,
#' running this function in parallel is not recommended.
#'
#' @param obj obj generated by [rpwf_add_model_info()].
#' @param db_con a [DbCon] object.
#'
#' @details
#' For each grid, initialize a new [RGrid], call `self$export()`, then return
#' the object. This make sure the same object called twice will just fetch
#' the result from the previous call. Hence, can't be run in parallel.
#'
#' At this point, we made sure that 1) the db is updated, 2) file is exported,
#' 3) file exists. Use the hashes for getting the grid id.
#'
#' @return tibble with the added "grid_id" column
#' @keywords internal
#' @export
rpwf_export_grid <- function(obj, db_con) {
  stopifnot("run rpwf_add_grid_param() first" = "grids" %in% names(obj))
  RGrid_obj <- lapply(obj$grids, \(x) {
    return(RGrid$new(x, db_con)$export())
  })

  query_res <- rpwf_query( # Use the hash to find the grid_id
    query = "SELECT grid_id FROM r_grid_tbl WHERE grid_hash = ?",
    con = db_con$con,
    val1 = sapply(RGrid_obj, \(x) {
      x$hash
    })
  )
  # Add `grid_id` to the accumulating object
  return(dplyr::mutate(obj, grid_id = query_res))
}


#' Process the train/test Data
#'
#' Add the [TrainDf] R6 objects using provided recipes. Similar to [rpwf_export_grid()],
#' this object does the heavy lifting of generating the path, parquet, and
#' updating the database of the recipe transformed data. Since the function is
#' reading from the database and only generating data as needed, running this
#' function in parallel is not recommended.
#'
#' @param obj obj generated by [rpwf_add_model_info()].
#' @inheritParams rpwf_export_grid
#' @param seed random seed. To control for recipes such as down sampling.
#'
#' @details
#' For each recipe, initialize a new [TrainDf], call `self$export()`, then return
#' the object. This make sure the same object called twice will just fetch the
#' result from the previous call.
#'
#' @return tibble with the added column "df_id".
#' @keywords internal
#' @export
rpwf_export_df <- function(obj, db_con, seed = 1234) {
  TrainDf_obj <- lapply(obj$preprocs, \(x) {
    set.seed(seed)
    return(TrainDf$new(x, db_con)$export())
  })

  query_res <- rpwf_query(
    con = db_con$con,
    query = "SELECT df_id FROM df_tbl WHERE df_hash = ?",
    val1 = sapply(TrainDf_obj, \(x) {
      x$hash
    })
  )
  # Add `df_id` to the accumulating object
  return(dplyr::mutate(obj, df_id = query_res))
}


#' Query the `cost_id` Associated with the Requested Cost Function
#'
#' @param obj object generated by [rpwf_add_model_info()].
#' @inheritParams rpwf_dm_obj
#'
#' @return tibble with the "cost_id" column added.
#' @keywords internal
#' @export
rpwf_add_cost <- function(obj, con) {
  query_res <- rpwf_query(
    query = "SELECT cost_id FROM cost_tbl WHERE cost_name = ? AND model_mode = ?;",
    con = con,
    val1 = obj$costs,
    val2 = obj$model_mode
  )

  return(dplyr::mutate(obj, cost_id = query_res))
}


#' Query the `model_type` id for the Requested Model
#'
#' @param obj an object generated by [rpwf_add_model_info()].
#' @inheritParams rpwf_dm_obj
#'
#' @return tibble with the "model_type_id" column added.
#' @keywords internal
#' @export
rpwf_add_model_type <- function(obj, con) {
  stopifnot(
    "add set_py_engine() to your {parsnip} model_spec object" =
      all(c("py_module", "py_base_learner") %in% names(obj))
  )

  query_res <- rpwf_query(
    query =
      "SELECT model_type_id FROM model_type_tbl
      WHERE py_module = ? AND py_base_learner = ?",
    con = con,
    val1 = obj$py_module,
    val2 = obj$py_base_learner
  )
  # Add `model_type_id` to the accumulating object
  return(dplyr::mutate(obj, model_type_id = query_res))
}


#' Add the Random State Seeds for Python `random_state`
#'
#' @param obj an object generated by [rpwf_add_model_info()].
#' @param range range of seed to sample from.
#' @param seed random seed to sample the `random_state`.
#'
#' @return tibble with "random_state" column added.
#' @keywords internal
#' @export
rpwf_add_random_state <- function(obj, range, seed) {
  set.seed(seed)
  stopifnot("range of random_state should be of length 2" = length(range) == 2L)
  sorted_range <- as.integer(sort(range))
  stopifnot("range should be an int vector" = !anyNA(sorted_range))
  random_state <- sample(sorted_range[1]:sorted_range[2], size = nrow(obj))
  # Add `random_state` to the accumulating object
  return(dplyr::mutate(obj, random_state = random_state))
}

#' Wrapper to Generate the Object to be Exported to the Database
#'
#' @param wflow_obj object created by the [rpwf_workflow_set()] function.
#' @inheritParams rpwf_export_grid
#' @inheritParams rpwf_grid_gen
#' @inheritParams rpwf_add_random_state
#'
#' @return tibble with the columns necessary for exporting to db.
#' @export
#' @examples
#' # Create the database
#' tmp <- tempdir()
#' db_con <- DbCon$new("db.SQLite", tmp)
#' rpwf_db_init(db_con$con, rpwf_schema())
#'
#' # Create a `workflow_set`
#' d <- rpwf_sim()$train
#' m1 <- parsnip::boost_tree() |>
#'   parsnip::set_engine("xgboost") |>
#'   parsnip::set_mode("classification") |>
#'   set_py_engine(py_module = "xgboost", py_base_learner = "XGBClassifier")
#' r1 <- d |>
#'   recipes::recipe(target ~ .) |>
#'   recipes::step_dummy(.data$X3, one_hot = TRUE) |>
#'   # "pd.index" is the special column that used for indexing in pandas
#'   recipes::update_role(.data$id, new_role = "pd.index")
#' wf <- rpwf_workflow_set(list(r1), list(m1), "neg_log_loss")
#'
#' to_export <- wf |>
#'   rpwf_augment(db_con, dials::grid_latin_hypercube, size = 10)
#' list.files(paste0(tmp, "/rpwfDb"), recursive = TRUE) # Files are created
rpwf_augment <- function(wflow_obj, db_con, .grid_fun = NULL,
                         ..., range = c(1L, 5000L), seed = 1234L) {
  set.seed(seed)
  wflow_obj |>
    rpwf_add_model_info(db_con$con) |>
    rpwf_add_desc() |>
    rpwf_add_grid_param(.grid_fun, seed, ...) |>
    # rpwf_export_grid(db_con) |>
    # rpwf_export_df(db_con, seed) |>
    rpwf_add_cost(db_con$con) |>
    rpwf_add_model_type(db_con$con) |>
    rpwf_add_random_state(range, seed)
}


#' Hash the Rows of the Workflow to Check if its Already in the db
#'
#' Export the object that has been processed by the other functions into the
#' database so that python can query the information.
#'
#' @param df object created by [rpwf_augment()].
#'
#' @return a vector of [rlang::hash].
#' @keywords internal
#' @export
rpwf_wflow_hash_ <- function(df) {
  apply(as.data.frame(df), 1, rlang::hash)
}

#' Export the [rpwf_augment()] Object into the Database
#'
#' @param obj object generated by [rpwf_augment()].
#' @inheritParams rpwf_dm_obj
#'
#' @return number of rows exported.
#' @export
#' @examples
#' # Create the database
#' db_con <- DbCon$new("db.SQLite", tempdir())
#' rpwf_db_init(db_con$con, rpwf_schema())
#'
#' # Create a `workflow_set`
#' d <- rpwf_sim()$train
#' m1 <- parsnip::boost_tree() |>
#'   parsnip::set_engine("xgboost") |>
#'   parsnip::set_mode("classification") |>
#'   set_py_engine(py_module = "xgboost", py_base_learner = "XGBClassifier")
#' r1 <- d |>
#'   recipes::recipe(target ~ .) |>
#'   recipes::step_dummy(.data$X3, one_hot = TRUE) |>
#'   # "pd.index" is the special column that used for indexing in pandas
#'   recipes::update_role(.data$id, new_role = "pd.index")
#' wf <- rpwf_workflow_set(list(r1), list(m1), "neg_log_loss")
#'
#' to_export <- wf |>
#'   rpwf_augment(db_con, dials::grid_latin_hypercube, size = 10) |>
#'   rpwf_export_grid(db_con) |>
#'   rpwf_export_df(db_con)
#'
#' # Before exporting
#' DBI::dbGetQuery(db_con$con, "SELECT * FROM wflow_tbl;")
#' rpwf_export_db(to_export, db_con$con)
#' # After exporting
#' DBI::dbGetQuery(db_con$con, "SELECT * FROM wflow_tbl;")
rpwf_export_db <- function(obj, con) {
  # These columns must be present in the data
  required <- c(
    "df_id", "grid_id", "wflow_desc", "cost_id", "model_type_id",
    "random_state", "py_base_learner_args"
  )
  hash_chk <- setdiff(required, "wflow_desc") # columns for hash checking
  # which mandatory column is not in the processed data?
  missing_cols <- required[which(!required %in% names(obj))]
  if (any(c("df_id", "grid_id") %in% missing_cols)) {
    stop("Run `rpwf_export_grid()` and `rpwf_export_df()` to write parquet files first")
  }

  if (length(missing_cols) != 0L) {
    # Which columns are not found in the object
    stop(glue::glue('{paste(missing_cols, collapse = ", ")} not found'))
  }

  # Query the wflow that's already in the database
  db_wflow_hash <- rpwf_wflow_hash_(
    dplyr::select(
      DBI::dbGetQuery(con, glue::glue("SELECT * FROM wflow_tbl;")),
      dplyr::all_of(hash_chk)
    )
  )

  to_export_hash <- rpwf_wflow_hash_(dplyr::select(obj, dplyr::all_of(hash_chk)))
  matched_wflow <- to_export_hash %in% db_wflow_hash
  if (any(matched_wflow)) {
    message("the following workflows are already in the database\n")
    print(obj[matched_wflow, which(names(obj) %in% required)])
  }
  # Only add the workflow that's not in the database
  to_export <- as.data.frame(obj[!matched_wflow, which(names(obj) %in% required)])

  if (nrow(to_export) == 0) {
    message("All workflows found in db, exiting...")
    return(0)
  } else {
    message("Exporting workflows to db...")
    DBI::dbAppendTable(con, name = "wflow_tbl", value = to_export)
  }
}
